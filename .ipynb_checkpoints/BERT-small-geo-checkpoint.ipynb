{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-30 20:21:14,589\tINFO -- MainProcess corpus.py:17 -- Welcome to the Georgian NLP toolset demo\n"
     ]
    }
   ],
   "source": [
    "# კორპუსთან სამუშაო ბიბლიოთეკის იმპორტი\n",
    "from GNLP.corpus import Corpus\n",
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-30 20:21:14,595\tINFO -- MainProcess corpus.py:113 -- ვიწყებ data/corpuses/kat-ge_web_2019_1M/kat-ge_web_2019_1M-sentences.txt-ის დამუშავებას\n",
      "2020-10-30 20:21:15,715\tINFO -- MainProcess corpus.py:126 -- წინადადების რაოდენობა: 1000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ვებ სკრაპი\n",
    "corpus.from_file('data/corpuses/kat-ge_web_2019_1M/kat-ge_web_2019_1M-sentences.txt')\n",
    "corpus.file2sequence()\n",
    "len(corpus.sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-30 20:21:15,785\tINFO -- MainProcess corpus.py:113 -- ვიწყებ data/corpuses/kat_newscrawl_2016_1M/kat_newscrawl_2016_1M-sentences.txt-ის დამუშავებას\n",
      "2020-10-30 20:21:17,048\tINFO -- MainProcess corpus.py:126 -- წინადადების რაოდენობა: 2000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2016 წლისთვის ვებ ნიუსები\n",
    "corpus.from_file('data/corpuses/kat_newscrawl_2016_1M/kat_newscrawl_2016_1M-sentences.txt')\n",
    "corpus.file2sequence()\n",
    "len(corpus.sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-30 20:21:17,057\tINFO -- MainProcess corpus.py:113 -- ვიწყებ data/corpuses/kawiki-latest-pages-articles_preprocessed.txt-ის დამუშავებას\n",
      "2020-10-30 20:21:18,814\tINFO -- MainProcess corpus.py:126 -- წინადადების რაოდენობა: 3657281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3657281"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# უახლესი ქართული ვიკიპედია\n",
    "corpus.from_file('data/corpuses/kawiki-latest-pages-articles_preprocessed.txt')\n",
    "corpus.file2sequence()\n",
    "len(corpus.sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3657281"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.preprocess_sequence()\n",
    "len(corpus.prepro_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prepro.txt', 'w', encoding='utf-8') as file:\n",
    "    for sentence in corpus.prepro_sequence:\n",
    "        file.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths: ['prepro.txt', 'requirements.txt']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\"./\").glob(\"*.txt\")]\n",
    "\n",
    "print(f'paths: {paths}')\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./QartBERT-vocab.json', './QartBERT-merges.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save files to disk\n",
    "tokenizer.save_model(\".\", \"QartBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./QartBERT-vocab.json\",\n",
    "    \"./QartBERT-merges.txt\",\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "print(\n",
    "    tokenizer.encode(\"ვინ ხარ შენ?\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>ვინ ხარ შენ?</s>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "decoder = ByteLevel()\n",
    "toks = tokenizer.encode(\"ვინ ხარ შენ?\").tokens\n",
    "\n",
    "decoder.decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['დაემატა თქვენ კალათაში\\n', 'მაისურაძე წიგნის\\n', 'რიგის ტიპი პირველადი მეორეული შეფუთვის მარკირების დიზაინის სპეციფიკაციაში ცვლილების რეგისტრაცია\\n', 'საათზე გაიხსნა უბნები ოპოზიციური პარტიებისგან გაკეთებული განცხადებები არის სიცრუე არის ტყუილი\\n', 'ფინალურ ეტაპზე მეტოქეს ელოდება აჭარაბეთი რომელმაც ვითიბი ბანკი დაამარცხა\\n', 'ქილის მოცულობა კუნელის ახალი ყვავილებით აავსეთ ბოლომდე არყით აავსეთ კვირა დააყენეთ\\n', 'ჭიქა მარტივი სიროფი ჭიქა წყალი ჭიქა შაქარი\\n', 'წამი ეწოდება მელოდრამას რომელშიც დავით ქაცარავა ლუიზა ნერსისიანი მონაწილეობენ\\n', 'საათის მონაცემებით ამომრჩეველია ურნასთან მისული\\n', 'უფასო ოპერაცია უსინათლოებს\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('prepro.txt', 'r', encoding='utf-8') as file:\n",
    "    print(file.readlines()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 30 20:54:17 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla M10           Off  | 00000000:3D:00.0 Off |                  N/A |\r\n",
      "| N/A   30C    P0    16W /  53W |      0MiB /  8129MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla M10           Off  | 00000000:3E:00.0 Off |                  N/A |\r\n",
      "| N/A   30C    P0    17W /  53W |      0MiB /  8129MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla M10           Off  | 00000000:3F:00.0 Off |                  N/A |\r\n",
      "| N/A   21C    P0    16W /  53W |      0MiB /  8129MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla M10           Off  | 00000000:40:00.0 Off |                  N/A |\r\n",
      "| N/A   24C    P0    16W /  53W |      0MiB /  8129MiB |      1%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-wjfezc9l\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 682 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from transformers==3.4.0) (0.7)\n",
      "Requirement already satisfied: packaging in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from transformers==3.4.0) (20.4)\n",
      "Collecting tokenizers==0.9.2\n",
      "  Downloading tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from transformers==3.4.0) (1.19.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.10.28-cp36-cp36m-manylinux2010_x86_64.whl (666 kB)\n",
      "\u001b[K     |████████████████████████████████| 666 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from transformers==3.4.0) (3.13.0)\n",
      "Requirement already satisfied: requests in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from transformers==3.4.0) (2.24.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: six in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from sacremoses->transformers==3.4.0) (1.15.0)\n",
      "Requirement already satisfied: click in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
      "Collecting joblib\n",
      "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from protobuf->transformers==3.4.0) (50.3.0.post20201006)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from requests->transformers==3.4.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from requests->transformers==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from requests->transformers==3.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages (from requests->transformers==3.4.0) (1.25.11)\n",
      "Building wheels for collected packages: transformers, sacremoses\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-3.4.0-py3-none-any.whl size=1285604 sha256=b5ebf0752265e91093ec2ae0afae487438704300a713cce2715dbd1f5040cb3f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ywu22d0a/wheels/5a/0a/d0/eb8d0ea1d7d02156f8675d6e5dfa52c03601cbe377290db8dc\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=a45466ea01408390d31c55b6290efb58cfec01cc2065276624df1734360a5a4a\n",
      "  Stored in directory: /home/tchichua/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built transformers sacremoses\n",
      "Installing collected packages: regex, joblib, tqdm, sacremoses, tokenizers, sentencepiece, filelock, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.3\n",
      "    Uninstalling tokenizers-0.9.3:\n",
      "      Successfully uninstalled tokenizers-0.9.3\n",
      "Successfully installed filelock-3.0.12 joblib-0.17.0 regex-2020.10.28 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.2 tqdm-4.51.0 transformers-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./QartBERT\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83504416"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 53s, sys: 24.6 s, total: 15min 17s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./prepro.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages/transformers/trainer.py:281: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./QartBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/home/tchichua/anaconda3/envs/env3.6/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='259' max='56356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  259/56356 03:39 < 13:18:22, 1.17 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "trainer.save_model(\"./QartBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
