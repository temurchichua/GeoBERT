{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 13:15:29,593\tINFO -- MainProcess corpus.py:17 -- Welcome to the Georgian NLP toolset demo\n"
     ]
    }
   ],
   "source": [
    "from GNLP.corpus import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{   'counted': 0,\n",
       "    'prepro_sequence': 0,\n",
       "    'sequence': 0,\n",
       "    'stop_words': 373,\n",
       "    'tokens': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp = Corpus()\n",
    "corp.from_file('data/corpuses/kawiki-latest-pages-articles_preprocessed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 13:15:29,614\tINFO -- MainProcess corpus.py:113 -- ვიწყებ data/corpuses/kawiki-latest-pages-articles_preprocessed.txt-ის დამუშავებას\n",
      "2020-10-21 13:15:29,709\tINFO -- MainProcess corpus.py:126 -- წინადადების რაოდენობა: 74233\n"
     ]
    }
   ],
   "source": [
    "corp.file2sequence(preprocess=True)\n",
    "corp.count_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corp.count_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120830"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corp.counted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'თბილისი'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.correction(\"თბილასი\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
    "#!unzip GNLP/data/bert_model/multi_cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_VOCAB = 'GNLP/multi_cased_L-12_H-768_A-12/vocab.txt'\n",
    "BERT_INIT_CHKPNT = 'GNLP/multi_cased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "BERT_CONFIG = 'GNLP/multi_cased_L-12_H-768_A-12/bert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install bert-tensorflow \n",
    "# conda install tensorflow = 1.14\n",
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "# !wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
    "# !wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
    "# !wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 13:15:33,927\tWARNING -- MainProcess module_wrapper.py:139 -- From /home/tchichua/GeoBERT/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import modeling\n",
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "\n",
    "    def __init__(self, do_lower_case=True, never_split=None):\n",
    "        if never_split is None:\n",
    "            never_split = []\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = never_split\n",
    "\n",
    "    def tokenize(self, text, never_split=None):\n",
    "        never_split = self.never_split + (never_split if never_split is not None else [])\n",
    "        text = self._clean_text(text)\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if token not in never_split:\n",
    "                if self.do_lower_case:\n",
    "                    token = token.lower()\n",
    "                token = self._run_strip_accents(token)\n",
    "                split_tokens.extend(self._run_split_on_punc(token))\n",
    "            else:\n",
    "                split_tokens.append(token)\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text, never_split=None):\n",
    "        if never_split is not None and text in never_split:\n",
    "            return [text]\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "    \n",
    "def _is_control(char):\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    cp = ord(char)\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.tokenization import WordpieceTokenizer, load_vocab, convert_by_vocab\n",
    "\n",
    "class FullTokenizer(object):\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case, \n",
    "                                              never_split = ['[CLS]', '[MASK]', '[SEP]'])\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return convert_by_vocab(self.inv_vocab, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=BERT_VOCAB, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ეს **mask** ვინ დაანაგვიანა [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '[CLS] ეს ქუჩვს ვინ დაანაგვიანა [SEP]'\n",
    "text_mask = text.replace('ქუჩვს', '**mask**')\n",
    "text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(mask, word):\n",
    "    splitted = mask.split('**mask**')\n",
    "    left = tokenizer.tokenize(splitted[0])\n",
    "    middle = tokenizer.tokenize(word)\n",
    "    right = tokenizer.tokenize(splitted[1])\n",
    "    indices = [i for i in range(len(left))]\n",
    "    for i in range(len(right)):\n",
    "        indices.append(i + len(middle) + len(left))\n",
    "    \n",
    "    indices = indices[1:-1]\n",
    "    tokenized = tokenizer.tokenize(mask.replace('**mask**',word))\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    ids_left = tokenizer.convert_tokens_to_ids(left)\n",
    "    ids_right = tokenizer.convert_tokens_to_ids(right)\n",
    "    indices_word = ids_left + ids_right\n",
    "    return ids, indices, indices_word[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ქუჩას', 'ქუჩის']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_states = corp.edit_candidates('ქუჩვს')\n",
    "possible_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101,\n",
       "  25579,\n",
       "  1585,\n",
       "  32961,\n",
       "  111518,\n",
       "  30536,\n",
       "  1569,\n",
       "  73701,\n",
       "  10871,\n",
       "  93362,\n",
       "  44970,\n",
       "  59081,\n",
       "  19060,\n",
       "  102],\n",
       " [101,\n",
       "  25579,\n",
       "  1585,\n",
       "  32961,\n",
       "  111518,\n",
       "  13282,\n",
       "  1569,\n",
       "  73701,\n",
       "  10871,\n",
       "  93362,\n",
       "  44970,\n",
       "  59081,\n",
       "  19060,\n",
       "  102])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [get_indices(text_mask, word) for word in possible_states]\n",
    "ids, seq_ids, word_ids = list(zip(*indices))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 13:15:34,331\tWARNING -- MainProcess module_wrapper.py:139 -- From /home/tchichua/GeoBERT/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.X = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "        \n",
    "        model = modeling.BertModel(\n",
    "            config=bert_config,\n",
    "            is_training=False,\n",
    "            input_ids=self.X,\n",
    "            use_one_hot_embeddings=False)\n",
    "        \n",
    "        output_layer = model.get_sequence_output()\n",
    "        embedding = model.get_embedding_table()\n",
    "        \n",
    "        with tf.variable_scope('cls/predictions'):\n",
    "            with tf.variable_scope('transform'):\n",
    "                input_tensor = tf.layers.dense(\n",
    "                    output_layer,\n",
    "                    units = bert_config.hidden_size,\n",
    "                    activation = modeling.get_activation(bert_config.hidden_act),\n",
    "                    kernel_initializer = modeling.create_initializer(\n",
    "                        bert_config.initializer_range\n",
    "                    ),\n",
    "                )\n",
    "                input_tensor = modeling.layer_norm(input_tensor)\n",
    "            \n",
    "            output_bias = tf.get_variable(\n",
    "            'output_bias',\n",
    "            shape = [bert_config.vocab_size],\n",
    "            initializer = tf.zeros_initializer(),\n",
    "            )\n",
    "            logits = tf.matmul(input_tensor, embedding, transpose_b = True)\n",
    "            self.logits = tf.nn.bias_add(logits, output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 13:15:34,403\tWARNING -- MainProcess module_wrapper.py:139 -- From /home/tchichua/GeoBERT/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "2020-10-21 13:15:34,406\tWARNING -- MainProcess module_wrapper.py:139 -- From /home/tchichua/GeoBERT/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "2020-10-21 13:15:34,432\tWARNING -- MainProcess module_wrapper.py:139 -- From /home/tchichua/GeoBERT/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "2020-10-21 13:15:34,446\tWARNING -- MainProcess lazy_loader.py:50 -- \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "2020-10-21 13:15:34,941\tWARNING -- MainProcess deprecation.py:323 -- From /home/tchichua/GeoBERT/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "2020-10-21 13:15:34,944\tWARNING -- MainProcess deprecation.py:323 -- From /opt/anaconda/envs/env3.6/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "model = Model()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "var_lists = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32_ref>,\n",
       " <tf.Variable 'cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32_ref>,\n",
       " <tf.Variable 'cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>,\n",
       " <tf.Variable 'cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>,\n",
       " <tf.Variable 'cls/predictions/output_bias:0' shape=(119547,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'cls')\n",
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 13:15:37,634\tINFO -- MainProcess saver.py:1284 -- Restoring parameters from multi_cased_L-12_H-768_A-12/bert_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(var_list = var_lists + cls)\n",
    "saver.restore(sess, BERT_INIT_CHKPNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_padded = tf.keras.preprocessing.sequence.pad_sequences(ids,padding='post')\n",
    "masked_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 14, 119547)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sess.run(tf.nn.softmax(model.logits), feed_dict = {model.X: masked_padded})\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.94074315, 0.9292732]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for no, ids in enumerate(seq_ids):\n",
    "    scores.append(np.prod(preds[no, ids, word_ids[no]]))\n",
    "    \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5030668, 0.4969332], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_scores = np.array(scores) / np.sum(scores)\n",
    "prob_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ქუჩას', 0.5030668), ('ქუჩის', 0.4969332)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = list(zip(possible_states, prob_scores))\n",
    "probs.sort(key = lambda x: x[1])  \n",
    "probs[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
